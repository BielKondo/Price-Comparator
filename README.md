# ML Scraper Tracker

API desenvolvida com FastAPI + SQLite + Pandas para realizar scraping de produtos do Mercado Livre, armazenar os dados em banco de dados relacional e gerar relatÃ³rios em Excel para anÃ¡lise posterior (ex: Power BI).

---

## ðŸŽ¯ Objetivo do Projeto

Este projeto foi desenvolvido com foco em:
* Praticar arquitetura backend organizada.
* Trabalhar com Web Scraping real.
* Persistir dados em banco relacional (SQLite).
* Criar uma API REST profissional com FastAPI.
* Gerar arquivos estruturados para anÃ¡lise no Power BI.
* Demonstrar conhecimento prÃ¡tico para portfÃ³lio.

**O sistema permite:**
* ðŸ”Ž Buscar produtos por palavra-chave.
* ðŸ’¾ Armazenar os resultados no banco.
* ðŸ“Š Exportar os dados para Excel.
* ðŸ“ˆ Gerar abas analÃ­ticas automaticamente.

---

## ðŸ—ï¸ Arquitetura do Projeto

```text
app/
 â”œâ”€â”€ api/
 â”‚   â””â”€â”€ routes/
 â”‚       â””â”€â”€ endpointsProd.py   # Endpoints da API
 â”œâ”€â”€ db/
 â”‚   â””â”€â”€ db.py                  # ConexÃ£o e queries SQLite
 â”œâ”€â”€ schemas/
 â”‚   â””â”€â”€ models.py              # Modelos Pydantic
 â”œâ”€â”€ services/
 â”‚   â”œâ”€â”€ web_scraper.py         # LÃ³gica de scraping
 â”‚   â””â”€â”€ export.py              # GeraÃ§Ã£o de Excel
 â””â”€â”€ app.py                     # InicializaÃ§Ã£o da aplicaÃ§Ã£o
```

### SeparaÃ§Ã£o de Responsabilidades:

* **API**: Expor endpoints REST.
* **Services**: Regras de negÃ³cio (scraping e exportaÃ§Ã£o).
* **DB**: PersistÃªncia e queries SQL.
* **Schemas**: ValidaÃ§Ã£o de dados com Pydantic.
* **App**: InicializaÃ§Ã£o e configuraÃ§Ã£o do servidor.

---

## ðŸ§  Tecnologias Utilizadas

### ðŸš€ Backend
* **FastAPI**: Framework ASGI moderno e performÃ¡tico.
* **Pydantic**: ValidaÃ§Ã£o de dados e schemas.
* **SQLite**: Banco de dados leve e integrado.
* **Uvicorn**: Servidor ASGI para rodar a aplicaÃ§Ã£o.

### ðŸ•¸ï¸ Web Scraping
* **Requests**: RealizaÃ§Ã£o de requisiÃ§Ãµes HTTP.
* **BeautifulSoup (bs4)**: Parsing e extraÃ§Ã£o de dados do HTML.

### ðŸ“Š AnÃ¡lise e ExportaÃ§Ã£o
* **Pandas**: ManipulaÃ§Ã£o e tratamento de dados.
* **OpenPyXL**: Engine para escrita de arquivos Excel (.xlsx).

---

## ðŸ—„ï¸ Estrutura do Banco de Dados (Tabela: products)

| Campo | Tipo | DescriÃ§Ã£o |
| :--- | :--- | :--- |
| id | INTEGER | Chave PrimÃ¡ria (Auto-incremento) |
| query | TEXT | Termo utilizado na busca |
| posicao | INTEGER | PosiÃ§Ã£o no ranking do ML |
| titulo | TEXT | Nome do produto |
| preco | INTEGER | Valor numÃ©rico limpo |
| produto_url | TEXT | Link direto da oferta |
| imagem_url | TEXT | Link da imagem miniatura |
| captured_at | DATETIME | Data e hora da extraÃ§Ã£o |

---

## ðŸ”Œ Endpoints da API

DocumentaÃ§Ã£o interativa disponÃ­vel em: `http://127.0.0.1:8000/docs`

* **POST /scrape**: Inicia a extraÃ§Ã£o. ParÃ¢metros: `q` (termo) e `pages` (qtd).
* **GET /products**: Retorna os itens do banco. Suporta filtros e limite.
* **GET /export**: Gera e baixa o relatÃ³rio analÃ­tico em Excel.

---

## ðŸ“Š RelatÃ³rio Excel Automatizado

O arquivo gerado pelo endpoint `/export` contÃ©m:
1. **Dados Brutos**: Tabela completa com todos os itens capturados.
2. **Top 20 Baratos**: Filtro automÃ¡tico dos melhores preÃ§os.
3. **Resumo AnalÃ­tico**: EstatÃ­sticas de PreÃ§o MÃ©dio, MÃ­nimo e MÃ¡ximo por busca.

---

## âš™ï¸ Como Executar o Projeto

**1. Clonar repositÃ³rio:**
`git clone https://github.com/seu-usuario/ml-scraper-tracker.git`

**2. Criar ambiente virtual:**
`python -m venv .venv`

**3. Ativar ambiente:**
* Windows: `.\.venv\Scripts\activate`
* Linux/Mac: `source .venv/bin/activate`

**4. Instalar dependÃªncias e rodar:**
`pip install -r requirements.txt`
`uvicorn app.app:app --reload`

---

## ðŸ“Œ Notas Importantes
> Este projeto possui fins estritamente educacionais. O funcionamento do scraper depende da estrutura
